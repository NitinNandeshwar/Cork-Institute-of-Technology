# -*- coding: utf-8 -*-
"""ML_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YcTgD80gWEHjYCMYSvLBU3-FnbtYolXb

# Assignment 2
## Nitin Nandeshwar
## R00183235
## 16/04/2020
"""

from google.colab import files
uploaded = files.upload()

# checking the file delimiter format
print (uploaded['Electric_Grid_Stability.csv'][:200].decode('utf-8') + '...')

"""# 1.Importing necessary libraries"""

import numpy as np 
import pandas as pd 
import io

import matplotlib.pyplot as plt
import seaborn as sns

"""# 2.Loading data"""

# loading the file in data frame
df= pd.read_csv(io.StringIO(uploaded['Electric_Grid_Stability.csv'].decode('utf-8')))

# Structure of dataset
df.info()

"""# 3.Data Preprocessing

* 3.1 Checking for duplicates
"""

print('Number of duplicates in dataset : ',sum(df.duplicated()))

"""* 3.2 Checking for missing values"""

print('Total number of missing values in dataset : ', df.isna().values.sum())

# get the number of missing data points per column
missing_values_count = df.isnull().sum()

# look at the # of missing points in the first ten columns
missing_values_count

# how many total missing values do we have?
total_cells = np.product(df.shape)
total_missing = missing_values_count.sum()

# percent of data that is missing
(total_missing/total_cells) * 100

"""* 3.3 Checking for class imbalance"""

# Barplot of Categorical variable stabf
plt.figure(figsize=(10,8))
plt.title('Barplot of Stability')
sns.countplot(df['stabf'])
plt.xticks(rotation=90)

# Distribution of factors in variable stabf
print('No Stability', round(df['stabf'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Stability', round(df['stabf'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

"""As we can see the data is not balanced it will affect the predicting model accuracy.

# 4.Exploratory Data Analysis
"""

# Correlation plot of all varaibles in Dataset
plt.figure(figsize=(20,10))
sns.heatmap(df.corr(),annot=True)

"""**Explanation**

So we can see that there is some relationship between “stab” ,” 'tau1' to 'tau4'”and” 'g1' to 'g4'”.variables 'p1' to 'p4' have high collinearity between them but they have very low correlation with “stab”. So the variable 'p1' to 'p4' will not effect the prediction of classification variable “stabf”.
"""

# Histogram of variable stabf with factors
facetgrid = sns.FacetGrid(df, hue='stabf', height=5,aspect=3)
facetgrid.map(sns.distplot,'stab', hist=True).add_legend()

"""**Explanation**

As we can see  it is some region in factors distribution where both stable and unstable factor share which makes the operator difficult to differentiate between the stability and unstability of electric grid.
"""

plt.figure(figsize=(10,7))
sns.boxplot(x='stabf', y='stab',data=df, showfliers=False)
plt.ylabel('stability mean')
plt.title("Boxplot of stability column across various activities")
plt.xticks(rotation=90)

"""As we can see the median is not same for both stability factor for differential equation root "stab".So their is some information pattern exsist betweendifferential equation root and stability factor."""

# Boxplot of ge vs stabf variable
plt.figure(figsize=(10,7))
sns.boxplot(x='stabf', y="g3",data=df, showfliers=False)

"""As we can see the median is not same for both stability factor for price elasticity coefficient "g3".So their is some information pattern exsist between price elasticity coefficient and stability factor.

# 5.ML models

* 5.1 Defining the train and target
"""

X=df.drop(['stabf'],axis=1)
y=df['stabf']

from sklearn.model_selection import train_test_split

# Split the data into 80% training and 20% testing
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)

print('Training data size : ', X_train.shape)
print('Test data size : ', X_test.shape)

# Data balance
from imblearn.over_sampling import SMOTE

print("Before OverSampling, counts of label 'stable': {}".format(sum(y_train=="stable")))
print("Before OverSampling, counts of label 'unstable': {} \n".format(sum(y_train=="unstable")))

# SMOTE function for balaning the categorical data 
sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label 'stable': {}".format(sum(y_train_res=="stable")))
print("After OverSampling, counts of label 'unstable': {}".format(sum(y_train_res=="unstable")))

"""* 5.2 **Logistic regression model** with Hyperparameter tuning and cross validation"""

from sklearn. linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

import warnings
warnings.filterwarnings("ignore")

parameters = {'C':np.arange(10,61,10), 'penalty':['l2','l1']}
lr_classifier = LogisticRegression()
lr_classifier_rs = RandomizedSearchCV(lr_classifier, param_distributions=parameters, cv=10,random_state = 42)
lr_classifier_rs.fit(X_train_res, y_train_res)

# Data Imbalance
lr_classifier_rs1 = RandomizedSearchCV(lr_classifier, param_distributions=parameters, cv=10,random_state = 42)
lr_classifier_rs1.fit(X_train, y_train)

y_pred = lr_classifier_rs.predict(X_test)

y_pred1 = lr_classifier_rs1.predict(X_test)

# Imbalance  Data
lr_accuracy1 = accuracy_score(y_true=y_test, y_pred=y_pred1)
print("Data Imbalance result ")
print("Accuracy using Logistic Regression : ", lr_accuracy1)
cm = confusion_matrix(y_test.values,y_pred1)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

# Balance data
lr_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)
print("Balance data")
print("Accuracy using Logistic Regression : ", lr_accuracy)
cm = confusion_matrix(y_test.values,y_pred)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

def plot_confusion_matrix(cm,lables):
    fig, ax = plt.subplots(figsize=(12,8)) # for plotting confusion matrix as image
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
    yticks=np.arange(cm.shape[0]),
    xticklabels=lables, yticklabels=lables,
    ylabel='True label',
    xlabel='Predicted label')
    plt.xticks(rotation = 90)
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, int(cm[i, j]),ha="center", va="center",color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()

plot_confusion_matrix(cm, np.unique(y_pred))

#function to get best random search attributes
def get_best_randomsearch_results(model):
    print("Best estimator : ", model.best_estimator_)
    print("Best set of parameters : ", model.best_params_)
    print("Best score : ", model.best_score_)

# getting best random search attributes
get_best_randomsearch_results(lr_classifier_rs)

"""* 5.3  **Decision tree model** with Hyperparameter tuning and cross validation"""

from sklearn.tree import DecisionTreeClassifier

parameters = {'max_depth':np.arange(2,10,2)}
dt_classifier = DecisionTreeClassifier()
dt_classifier_rs = RandomizedSearchCV(dt_classifier,param_distributions=parameters,random_state = 42)
dt_classifier_rs.fit(X_train_res, y_train_res)

# Data Imbalance
dt_classifier_rs1 = RandomizedSearchCV(dt_classifier,param_distributions=parameters,random_state = 42)
dt_classifier_rs1.fit(X_train, y_train)

y_pred = dt_classifier_rs.predict(X_test)
# Data Imbalance prediction
y_pred1 = dt_classifier_rs1.predict(X_test)

# Data Imbalance 
dt_accuracy1 = accuracy_score(y_true=y_test, y_pred=y_pred1)
print("Data Imbalance result ")
print("Accuracy using Decision tree : ", dt_accuracy1)
cm = confusion_matrix(y_test.values,y_pred1)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

# Balance data
dt_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)
print("Balance Data")
print("Accuracy using Decision tree : ", dt_accuracy)
cm = confusion_matrix(y_test.values,y_pred)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

plot_confusion_matrix(cm, np.unique(y_pred))

# getting best random search attributes
get_best_randomsearch_results(dt_classifier_rs)

"""* 5.4 **Linear SVM model** with Hyperparameter tuning and cross validation"""

from sklearn.svm import LinearSVC

parameters = {'C':np.arange(1,12,2)}
lr_svm = LinearSVC(tol=0.00005)
lr_svm_rs = RandomizedSearchCV(lr_svm, param_distributions=parameters,random_state = 42)
lr_svm_rs.fit(X_train_res, y_train_res)

# Data Imbalance
lr_svm_rs1 = RandomizedSearchCV(lr_svm, param_distributions=parameters,random_state = 42)
lr_svm_rs1.fit(X_train, y_train)

y_pred = lr_svm_rs.predict(X_test)
# Data Imbalance prediction
y_pred1 = lr_svm_rs1.predict(X_test)

# Data Imbalance
lr_svm_accuracy1 = accuracy_score(y_true=y_test, y_pred=y_pred1)
print("# Data Imbalance")
print("Accuracy using linear SVM : ",lr_svm_accuracy1)
cm = confusion_matrix(y_test.values,y_pred1)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

# Balance data
lr_svm_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)
print("Balance Data")
print("Accuracy using linear SVM : ",lr_svm_accuracy)
cm = confusion_matrix(y_test.values,y_pred)
print("Confusion Matrix:")
print(cm)
TP =cm[0][0]
TN =cm[1][1]
FN = cm[1][0]
FP = cm[0][1] 

precision=TP/(TP+FP)
recall= TP/(TP+FN)
F1=(2*precision*recall)/(precision+recall)
print("precision:",precision)
print("Recall:",recall)
print("F1_score:",F1)

plot_confusion_matrix(cm, np.unique(y_pred)) # plotting confusion matrix

# getting best random search attributes
get_best_randomsearch_results(lr_svm_rs)
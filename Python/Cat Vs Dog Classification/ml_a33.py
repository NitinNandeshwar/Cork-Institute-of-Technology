# -*- coding: utf-8 -*-
"""ML_A33.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQdFp9F4NiELd1tv7raYrxYkZDZJyiAw

# Assignment 3
## Nitin Nandeshwar
## R00183235
## 21/04/2020

# Google Drive file Access
"""

# Libraries to mount google drive files to google colab
#!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

#Function used to authorization to access file from google drive
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Accessing the Project3_files1.zip from google drive
fid = drive.ListFile({'q':"title='Project3_files1.zip'"}).GetList()[0]['id']
f = drive.CreateFile({'id': fid})
f.GetContentFile('Project3_files1.zip')

"""# Unzip the Project3 files"""

# unzip the Project3_files1.zip
from zipfile import ZipFile
file_name='Project3_files1.zip'

with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')

"""# List of Libraries"""

# Commented out IPython magic to ensure Python compatibility.
# Libraries used for Project3
import os, cv2, re, random
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline

"""# Data dimensions

-  **Loading** the train and test dataset
"""

# Setting the images of an equal size (eg. 350 *350 *3)
img_width = 150
img_height = 150
TRAIN_DIR = './Project3_files/data/train/'
TEST_DIR = './Project3_files/data/test/'
train_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # Train Dataset
test_images_dogs_cats = [TEST_DIR+i for i in os.listdir(TEST_DIR)]    # Test Dataset

"""**Explanation**

* For Image of equal size of 350* 350* 3 we have to set imh_width=350 and img_height= 350
* If used above setting of 350*350*3 taining data is taking very long time so here i used 150*150*3 size
"""

# Function to access image names
def atoi(text):
    return int(text) if text.isdigit() else text

def natural_keys(text):
    return [ atoi(c) for c in re.split('(\d+)', text) ]

# sorting the data of dog & cat
train_images_dogs_cats.sort(key=natural_keys)

test_images_dogs_cats.sort(key=natural_keys)

# Function for preparing the data
def prepare_data(list_of_images):
    """
    Returns two arrays: 
        x is an array of resized images
        y is an array of labels
    """
    x = [] # images as arrays
    y = [] # labels
    
    for image in list_of_images:
        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))
    
    for i in list_of_images:
        if 'dog' in i:
            y.append(1)
        elif 'cat' in i:
            y.append(0)
        #else:
            #print('neither cat nor dog name present in images')
            
    return x, y

# Prepare the data 
X, Y = prepare_data(train_images_dogs_cats)
X = np.asarray(X)
Y = np.asarray(Y)

# Dimension of Train data 
X.shape

# Barplot of Train data Dog & cat
plt.figure(figsize=(10,8))
plt.title('Barplot of dog & cat')
sns.countplot(Y)
plt.xticks(rotation=90)

"""**Explanation**
* As we can see the train data barplot for dog and cat is equally distributed so we can say the image classification data is balanced data.
"""

# Spliting the Train dat into 80% training data and 20% testing data
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=1)
print('Training data shape: ', X_train.shape)
print('Training labels shape: ', y_train.shape)
print('Test data shape: ', X_test.shape)
print('Test labels shape: ', y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function
import random
import numpy as np
import matplotlib.pyplot as plt


# This is a bit of magic to make matplotlib figures appear inline in the notebook
# rather than in a new window.
# %matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# %load_ext autoreload
# %autoreload 2

# Visualize some examples from the dataset.
# We show a few examples of training images from each class.
classes = ['cat','dog']
num_classes = len(classes)
samples_per_class = 7
for y, cls in enumerate(classes):
    idxs = np.flatnonzero(y_train == y)
    idxs = np.random.choice(idxs, samples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt_idx = i * num_classes + y + 1
        plt.subplot(samples_per_class, num_classes, plt_idx)
        plt.imshow(X_train[idx].astype('uint8'))
        plt.axis('off')
        if i == 0:
            plt.title(cls)
plt.show()

# Reshape the image data into rows
X_train = np.reshape(X_train, (X_train.shape[0], -1))
X_test = np.reshape(X_test, (X_test.shape[0], -1))
print(X_train.shape, X_test.shape)

# Dimension of Training and test data
print("x train: ",X_train.shape)
print("x test: ",X_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)

y_test.shape[0]

"""# SVM Model"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.svm import LinearSVC

# filter all the warnings
import warnings
warnings.filterwarnings('ignore')

lsvc = LinearSVC(random_state=0,tol=1e-5)
lsvc.fit(X_train,y_train)
print('Coef',lsvc.coef_)
print('Intercept',lsvc.intercept_)

# 2-fold cross validation
lsvc_score = lsvc.score(X_test,y_test)
print('Score', lsvc_score)
kfold = KFold(n_splits= 2, random_state= 9 )
cv_results = cross_val_score(lsvc , X_train, y_train, cv=kfold, scoring="accuracy")
print(cv_results)

pred=lsvc.predict(X_test)

plt.figure(figsize=(10,8))
plt.title('Barplot of dog vs cat')
sns.countplot(pred)
plt.xticks(rotation=90)

"""**Explanation**

* Accuracy of SVM for training data is 54%  using 5-fold cross validation.

# K Nearest Neighbor
"""

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors= 4)
model.fit(X_train, y_train)
acc = model.score(X_test,y_test)
print("pixel accuracy: {:.2f}%".format(acc * 100))

# predicted output for testing data
y_pred=model.predict(X_test)

# Bar plot of predicted testing data
plt.figure(figsize=(10,8))
plt.title('Barplot of dog vs cat')
sns.countplot(y_pred)
plt.xticks(rotation=90)

num_folds = 5
k_choices = [1, 3, 7 , 5, 8, 10]

X_train_folds = []
y_train_folds = []
################################################################################
# TODO:                                                                        #
# Split up the training data into folds. After splitting, X_train_folds and    #
# y_train_folds should each be lists of length num_folds, where                #
# y_train_folds[i] is the label vector for the points in X_train_folds[i].     #
# Hint: Look up the numpy array_split function.                                #
################################################################################
X_train_folds = np.array_split(X_train, num_folds)
y_train_folds = np.array_split(y_train, num_folds)
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

# A dictionary holding the accuracies for different values of k that we find
# when running cross-validation. After running cross-validation,
# k_to_accuracies[k] should be a list of length num_folds giving the different
# accuracy values that we found when using that value of k.
k_to_accuracies = {}


################################################################################
# TODO:                                                                        #
# Perform k-fold cross validation to find the best value of k. For each        #
# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #
# where in each case you use all but one of the folds as training data and the #
# last fold as a validation set. Store the accuracies for all fold and all     #
# values of k in the k_to_accuracies dictionary.                               #
################################################################################
for k in k_choices:
    k_to_accuracies[k] = []
    for i in range(num_folds):
        # prepare training data for the current fold
        X_train_fold = np.concatenate([ fold for j, fold in enumerate(X_train_folds) if i != j ])
        y_train_fold = np.concatenate([ fold for j, fold in enumerate(y_train_folds) if i != j ])
        
        # use of k-nearest-neighbor algorithm
        model.fit(X_train_fold, y_train_fold)
        y_pred_fold = model.predict(X_train_folds[i])

        # Compute the fraction of correctly predicted examples
        num_correct = np.sum(y_pred_fold == y_train_folds[i])
        accuracy = float(num_correct) / X_train_folds[i].shape[0]
        k_to_accuracies[k].append(accuracy)
        
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

# Print out the computed accuracies
for k in sorted(k_to_accuracies):
    for accuracy in k_to_accuracies[k]:
        print('k = %d, accuracy = %f' % (k, accuracy))

# plot the raw observations
plt.rcParams['figure.figsize'] = (10.0, 8.0)
for k in k_choices:
    accuracies = k_to_accuracies[k]
    plt.scatter([k] * len(accuracies), accuracies)

# plot the trend line with error bars that correspond to standard deviation
accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])
accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])
plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)
plt.title('Cross-validation on k')
plt.xlabel('k')
plt.ylabel('Cross-validation accuracy')
plt.show()

# Based on the cross-validation results above, choose the best value for k,   
# retrain the classifier using all the training data, and test it on the test
# data. You should be able to get above 53% accuracy on the test data.
best_k = k_choices[accuracies_mean.argmax()]

print("Best K value :",best_k)
#classifier = KNearestNeighbor()
model.fit(X_train, y_train)
y_test_pred = model.predict(X_test)

num_test=y_test.shape[0]
# Compute and display the accuracy
num_correct = np.sum(y_test_pred == y_test)
accuracy = float(num_correct) / num_test
print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))

# predicted output of testing data
y_test_pred

plt.figure(figsize=(10,8))
plt.title('Barplot of dog vs cat')
sns.countplot(y_test_pred)
plt.xticks(rotation=90)

"""# Test data Evaluation"""

# Function for prepareing data for test data
def test_prepare_data(list_of_images):
   
    x = [] # images as arrays
    y = [] # labels
    
    for image in list_of_images:
        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))
                 
    return x

# Preparing test data 
test_data= test_prepare_data(test_images_dogs_cats)
test_data = np.asarray(test_data)
print("Dimenson of test data:")
test_data.shape

# Reshape the test image data into rows
test_data = np.reshape(test_data, (test_data.shape[0], -1))
test_data.shape

print("Best K value :",best_k)
y_test_data_pred = model.predict(test_data)

y_test_data_pred

# Barplot of predicted Test data
plt.figure(figsize=(10,8))
plt.title('Barplot of dog vs cat')
sns.countplot(y_test_data_pred)
plt.xticks(rotation=90)